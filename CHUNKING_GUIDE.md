# Guide des m√©thodes de Chunking

Ce guide explique les diff√©rentes m√©thodes de chunking disponibles et comment les utiliser.

## Vue d'ensemble

Le chunking est l'√©tape qui d√©coupe vos documents en morceaux plus petits pour l'insertion dans ChromaDB. La qualit√© du chunking impacte directement la qualit√© des r√©sultats de recherche du syst√®me RAG.

## M√©thodes disponibles

### 1Ô∏è‚É£ **R√©cursive** (RECOMMAND√â) ‚≠ê

```python
CHUNK_METHOD = "recursive"
```

**Description** : D√©coupe le texte en respectant la structure naturelle (paragraphes ‚Üí phrases ‚Üí mots)

**Avantages** :
- ‚úÖ Respecte la structure du document
- ‚úÖ Chunks s√©mantiquement coh√©rents
- ‚úÖ Pas de d√©pendances suppl√©mentaires
- ‚úÖ Bon compromis performance/qualit√©

**Inconv√©nients** :
- Taille de chunks variable

**Utilisation** :
```python
from config import CHUNK_SIZE, CHUNK_OVERLAP
from text_utils_recursive import chunk_text_recursive

chunks = chunk_text_recursive(text, CHUNK_SIZE, CHUNK_OVERLAP)
```

---

### 2Ô∏è‚É£ **Fixe** (M√©thode actuelle)

```python
CHUNK_METHOD = "fixed"
```

**Description** : D√©coupe par taille fixe avec d√©tection basique des limites de mots

**Avantages** :
- ‚úÖ Simple et rapide
- ‚úÖ Taille pr√©visible

**Inconv√©nients** :
- ‚ùå Peut couper au milieu des phrases
- ‚ùå Ne respecte pas la structure s√©mantique
- ‚ùå Qualit√© de recherche inf√©rieure

**Utilisation** :
```python
from text_utils import chunk_text

chunks = chunk_text(text, chunk_size=500, overlap=50)
```

---

### 3Ô∏è‚É£ **S√©mantique** (Avanc√©) üöÄ

```python
CHUNK_METHOD = "semantic"
```

**Description** : D√©tecte automatiquement les ruptures s√©mantiques en analysant la similarit√© entre phrases

**Installation** :
```bash
pip install sentence-transformers
```

**Avantages** :
- ‚úÖ Meilleure qualit√© : d√©tecte les changements de sujet
- ‚úÖ Chunks naturellement coh√©rents
- ‚úÖ Id√©al pour documents complexes

**Inconv√©nients** :
- ‚ö†Ô∏è Plus lent (calcul d'embeddings)
- ‚ö†Ô∏è D√©pendance suppl√©mentaire
- ‚ö†Ô∏è Taille de chunks tr√®s variable

**Utilisation** :
```python
from text_utils_semantic import chunk_text_semantic

chunks = chunk_text_semantic(text, max_chunk_size=500, threshold=0.5)
```

**Param√®tres** :
- `threshold` : Seuil de rupture s√©mantique (0-1)
  - Plus haut (0.7-0.9) = chunks plus petits, plus focalis√©s
  - Plus bas (0.3-0.5) = chunks plus grands, plus de contexte

---

### 4Ô∏è‚É£ **LangChain** (Production) üè≠

```python
CHUNK_METHOD = "langchain"
```

**Description** : Utilise les text splitters de LangChain, solution production-ready

**Installation** :
```bash
pip install langchain langchain-text-splitters
```

**Avantages** :
- ‚úÖ Bien test√© et maintenu
- ‚úÖ Compatible √©cosyst√®me LangChain
- ‚úÖ Multiple strat√©gies disponibles

**Inconv√©nients** :
- ‚ö†Ô∏è D√©pendance suppl√©mentaire (assez lourde)

**Utilisation** :
```python
from text_utils_langchain import chunk_text_langchain

# M√©thode r√©cursive
chunks = chunk_text_langchain(text, method="recursive")

# Par tokens
chunks = chunk_text_langchain(text, method="token")
```

---

## Configuration

### M√©thode simple : √âditer `config.py`

```python
# config.py

# Choisir la m√©thode
CHUNK_METHOD = "recursive"  # ou "fixed", "semantic", "langchain"

# Param√®tres g√©n√©raux
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

# Pour chunking s√©mantique
SEMANTIC_THRESHOLD = 0.5
```

### Utilisation du module unifi√©

```python
from text_utils_unified import chunk_text, prepare_chunks_for_db

# Utilise automatiquement la m√©thode de config.py
chunks = chunk_text(text)

# Ou sp√©cifier explicitement
chunks = chunk_text(text, method="recursive")
```

---

## Comparaison des m√©thodes

Pour comparer toutes les m√©thodes sur vos documents :

```bash
python3 compare_chunking_methods.py
```

Ce script affichera :
- Nombre de chunks par m√©thode
- Taille moyenne/min/max
- Exemples de chunks
- Recommandations

---

## Migration depuis le syst√®me actuel

### Option 1 : Utiliser le module unifi√© (Recommand√©)

1. **√âditer `main.py`** :

```python
# Remplacer
from text_utils import prepare_chunks_for_db

# Par
from text_utils_unified import prepare_chunks_for_db
```

2. **Configurer dans `config.py`** :
```python
CHUNK_METHOD = "recursive"
```

3. **Recr√©er la base** :
```bash
python3 main.py
```

### Option 2 : Modification manuelle

√âditer directement `main.py` pour utiliser une m√©thode sp√©cifique :

```python
# Importer la m√©thode choisie
from text_utils_recursive import prepare_chunks_for_db

# Ou pour s√©mantique
from text_utils_semantic import prepare_chunks_for_db_semantic as prepare_chunks_for_db

# Le reste du code reste identique
```

---

## Recommandations par type de document

| Type de document | M√©thode recommand√©e | Raison |
|------------------|---------------------|---------|
| Documentation technique | **R√©cursive** | Structure claire (sections, paragraphes) |
| Articles de blog | **R√©cursive** | Bon √©quilibre qualit√©/performance |
| Livres, longs textes | **S√©mantique** | D√©tecte les changements de chapitres/th√®mes |
| Code source | **LangChain-token** | Respect de la syntaxe |
| Conversations, chats | **Fixe** | Pas de structure forte |
| Documents scientifiques | **S√©mantique** | Transitions complexes entre sujets |

---

## Param√®tres recommand√©s

### Pour documents courts (< 5000 caract√®res)
```python
CHUNK_SIZE = 300
CHUNK_OVERLAP = 30
CHUNK_METHOD = "recursive"
```

### Pour documents moyens (5000-50000 caract√®res)
```python
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
CHUNK_METHOD = "recursive"
```

### Pour documents longs (> 50000 caract√®res)
```python
CHUNK_SIZE = 800
CHUNK_OVERLAP = 100
CHUNK_METHOD = "semantic"
SEMANTIC_THRESHOLD = 0.6
```

### Pour performance maximale
```python
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
CHUNK_METHOD = "recursive"  # √âviter "semantic"
```

---

## D√©pannage

### Chunks trop grands
- R√©duire `CHUNK_SIZE`
- Utiliser m√©thode "recursive" ou "semantic"

### Chunks trop petits
- Augmenter `CHUNK_SIZE`
- Pour s√©mantique : r√©duire `SEMANTIC_THRESHOLD`

### R√©sultats de recherche de mauvaise qualit√©
- Essayer m√©thode "recursive" ou "semantic"
- Augmenter `CHUNK_OVERLAP` (50-100)
- V√©rifier que les chunks ne coupent pas les phrases

### Performance lente
- √âviter m√©thode "semantic" pour gros volumes
- Utiliser "recursive" (meilleur compromis)
- R√©duire le nombre de documents

---

## Exemple complet

```python
#!/usr/bin/env python3
from document_loader import load_documents
from text_utils_unified import prepare_chunks_for_db
from chroma_manager import ChromaDBManager
from config import DOCUMENTS_DIR, CHROMA_DB_PATH, COLLECTION_NAME

# Charger les documents
documents = load_documents(DOCUMENTS_DIR)

# Cr√©er les chunks (utilise CHUNK_METHOD de config.py)
texts, metadatas, ids = prepare_chunks_for_db(documents)

# Ou sp√©cifier la m√©thode
texts, metadatas, ids = prepare_chunks_for_db(documents, method="semantic")

# Ins√©rer dans ChromaDB
db = ChromaDBManager(CHROMA_DB_PATH, COLLECTION_NAME)
db.connect()
db.create_collection(reset=True)
db.insert_documents(texts, metadatas, ids)
db.close()
```

---

## Questions fr√©quentes

**Q : Dois-je recr√©er la base si je change de m√©thode ?**
R : Oui, les chunks seront diff√©rents donc il faut recr√©er avec `python3 main.py`

**Q : Quelle m√©thode est la plus rapide ?**
R : "fixed" > "recursive" >> "langchain" >> "semantic"

**Q : Quelle m√©thode donne les meilleurs r√©sultats ?**
R : "semantic" ‚â• "recursive" > "langchain" > "fixed"

**Q : Puis-je combiner plusieurs m√©thodes ?**
R : Non, mais vous pouvez cr√©er plusieurs collections avec diff√©rentes m√©thodes

**Q : La m√©thode affecte-t-elle les embeddings ?**
R : Non, seul le d√©coupage change. Les embeddings sont toujours g√©n√©r√©s par all-MiniLM-L6-v2
